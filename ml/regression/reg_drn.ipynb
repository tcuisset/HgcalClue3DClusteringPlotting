{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ntupleReaders.clue_ntuple_reader import ClueNtupleReader\n",
    "%aimport ml.regression.rechits\n",
    "from ml.regression.rechits import *\n",
    "from ml.regression.drn.modules import TracksterPropertiesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_sim = ClueNtupleReader(\"v40\", \"cmssw\", \"sim_proton_v46_patchMIP\")\n",
    "reader_data = ClueNtupleReader(reader_sim.version, reader_sim.clueParams, \"data\")\n",
    "#data_list = torch.load(os.path.join(reader_sim.pathToFolder, \"rechitsGeometric.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TracksterPropertiesDataset(reader_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[541, 4], beamEnergy=[1], trueBeamEnergy=[1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch size,batch_size: 287740 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 16:34:57.143256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 16:34:57.300873: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "totalev = len(dataset)\n",
    "ntrain = int(0.8*totalev)\n",
    "shuffled_dataset = dataset.shuffle()\n",
    "ntrainbatch = 200\n",
    "ntestbatch = 100\n",
    "trainloader = torch_geometric.loader.DataLoader(shuffled_dataset[:ntrain], batch_size=ntrainbatch)\n",
    "testloader = torch_geometric.loader.DataLoader(shuffled_dataset[ntrain:], batch_size=ntestbatch)\n",
    "#batch_size = ntrainbatch\n",
    "epoch_size = len(shuffled_dataset[:ntrain])\n",
    "print(\"epoch size,batch_size:\",epoch_size,ntrainbatch)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.dynamic_reduction_network import DynamicReductionNetwork\n",
    "from ml.cyclic_lr import CyclicLRWithRestarts\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drn = DynamicReductionNetwork(input_dim=4,hidden_dim=20,k=10,output_dim=1,norm=torch.tensor([ 1., 1., 1., 1.]))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        logits = self.drn(data)\n",
    "        #return F.softplus(logits)\n",
    "        return logits\n",
    "device = torch.device('cuda:1')#('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "scheduler = CyclicLRWithRestarts(optimizer, ntrainbatch, epoch_size, restart_period=80, t_mult=1.2, policy=\"cosine\")\n",
    "#criterion = torch.nn.MSELoss()\n",
    "\n",
    "#lossmse = nn.MSELoss()\n",
    "def resoloss2(output,truth):\n",
    "    \"\"\" Computes MSE loss between output and truth \"\"\"\n",
    "    batch_size = output.size()[0]\n",
    "    mse = F.mse_loss(output, truth, reduction='mean')\n",
    "    #mse = torch.sum((output-truth)**2/truth)/batch_size\n",
    "    #mse = torch.sum(((output-truth)/truth)**2)/batch_size\n",
    "    #mse = torch.mean(torch.abs((output - truth)))\n",
    "    \n",
    "    #res = \n",
    "    return (mse)\n",
    "\n",
    "#model.train()\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    loss = []\n",
    "    for data in tqdm(trainloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            result = model(data)\n",
    "            lossc = resoloss2(result, data.trueBeamEnergy)\n",
    "            loss.append(lossc.item()) \n",
    "            lossc.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.batch_step()\n",
    "    print( 'batches for train:',len(loss)) \n",
    "    print('train loss:',np.mean(np.array(loss)))\n",
    "    return np.mean(np.array(loss))\n",
    "\n",
    "from scipy.stats import norm\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "def gaussian(x,  mean,a, sigma):\n",
    "    return a * np.exp(-((x - mean)**2 / (2 * sigma**2)))\n",
    "\n",
    "def evaluate(epoch):\n",
    "        \"\"\"\"Evaluate the model\"\"\"\n",
    "        model.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        loss= []\n",
    "        \n",
    "        correct = 0\n",
    "        predc = []\n",
    "        truec = []\n",
    "        for data in tqdm(testloader):\n",
    "            data = data.to(device)        \n",
    "            result = model(data)\n",
    "            lossc = resoloss2(result, data.trueBeamEnergy)\n",
    "\n",
    "            loss.append(lossc.item())\n",
    "\n",
    "            for i in result:\n",
    "                pred.append(i.detach().cpu())\n",
    "                #predc.append(i.detach().cpu().argmax())\n",
    "            for i in data.trueBeamEnergy.detach():\n",
    "                true.append(i.detach().cpu())\n",
    "\n",
    "        print('batches for test:', len(loss)) \n",
    "        print('test loss:',np.mean(np.array(loss)))\n",
    "#        fracarr = np.array(frac\n",
    "\n",
    "        \n",
    "        preda = np.array(pred)\n",
    "        \"\"\" Array of predictions (beam energy) \"\"\"\n",
    "        truea = np.array(true)\n",
    "        \"\"\" Array of true beam energy \"\"\"\n",
    "        #preda = preda[:,2] ### added\n",
    "        #truea = truea[:,1] ### added\n",
    "        fracarr = (preda - truea)/truea\n",
    "        \"\"\" Array of (prediction - truth)/truth \"\"\"\n",
    "        #print(preda,truea,fracarr)\n",
    "        print('pred - true / true mean:',(np.mean(fracarr)))\n",
    "        print('pred - true / true std:',(np.std(fracarr)))\n",
    "        (mu, sigma) = norm.fit(fracarr)\n",
    "        \"\"\" gaussian fit of (prediction - truth)/truth \"\"\"\n",
    "        print('mu,sig:',mu,sigma)\n",
    "        \n",
    "\n",
    "        #bin_heights, bin_borders, _ = plt.hist(fracarr,range=[-2,2], bins=100, label='histogram')\n",
    "        #bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "        \n",
    "        #### Histogram of (pred - true) / true \n",
    "        from matplotlib.pyplot import figure\n",
    "        # fig = figure(figsize=(30, 20), dpi=40)\n",
    "        # plt.rcParams['axes.labelsize'] = 36\n",
    "        # plt.rcParams['axes.titlesize'] = 36\n",
    "        # plt.hist(fracarr,bins=100,range=[-4,4])\n",
    "        # plt.xticks(fontsize=16)\n",
    "        # plt.yticks(fontsize=16)\n",
    "        # plt.axvline(x=0.0,c='r')\n",
    "        #plt.show()\n",
    "        writer.add_histogram(\"Pred-truth / truth\", fracarr, epoch)\n",
    "\n",
    "        \n",
    "        '''\n",
    "        bins =  np.linspace(0,18,19)\n",
    "\n",
    "        fig, axs = plt.subplots(6,3, figsize=(40, 40), facecolor='w', edgecolor='k')\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        for i in tqdm(range (bins.size - 1)):\n",
    "            predaa = preda[(truea >bins[i]) & (truea <bins[i+1]) ]\n",
    "            trueaa = truea[(truea >bins[i]) & (truea <bins[i+1]) ]\n",
    "            fracarr = (predaa - trueaa)/trueaa\n",
    "            #if (fracarr < 0):\n",
    "            axs[i].hist(fracarr,bins=100,range=[-4,4],)\n",
    "            axs[i].set_xlabel('pred - true / true')\n",
    "            axs[i].set_ylabel('counts')\n",
    "            axs[i].set_title(str(bins[i])+\" to \"+str(bins[i+1]))\n",
    "            #print (vals)\n",
    "        if (epoch%5==0):\n",
    "            plt.savefig('%s/mt-mp_frac_ep%d.png'%(plot_dir, epoch), bbox_inches='tight')\n",
    "        plt.show()'''\n",
    "        \n",
    "        '''try:\n",
    "            from matplotlib.pyplot import figure\n",
    "            figure(figsize=(20, 10), dpi=30)\n",
    "            plt.rcParams['axes.labelsize'] = 16\n",
    "            plt.rcParams['axes.titlesize'] = 16\n",
    "            popt, _ = curve_fit(gaussian, bin_centers, bin_heights, p0=[0., 100., 1.],bounds = ([-np.inf,0,0],[np.inf,np.inf,np.inf]))\n",
    "            x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 100)\n",
    "            plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='fit')\n",
    "            plt.legend()\n",
    "\n",
    "\n",
    "            plt.xlabel('pred - true / true')\n",
    "            plt.ylabel('counts')\n",
    "            #plt.yscale(\"log\")\n",
    "            #plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "            plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(popt[0], popt[2]))\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        except RuntimeError:\n",
    "            print(\"Error - curve_fit failed\")\n",
    "            plt.xlabel('pred - true / true')\n",
    "            plt.ylabel('counts')\n",
    "            #plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "            plt.title('pred - true / true fit failed')\n",
    "            #plt.yscale(\"log\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.show()'''\n",
    "        ### Scatter of true beam energy (x) vs predicted beam energy (y)\n",
    "        from matplotlib.pyplot import figure\n",
    "        fig = figure(figsize=(30, 30), dpi=40)\n",
    "        #plt.rcParams['axes.labelsize'] = 36\n",
    "        #plt.rcParams['axes.titlesize'] = 36\n",
    "        #plt.hist2d(truea,preda,bins=200)\n",
    "        plt.scatter(truea,preda,alpha=0.4)\n",
    "        plt.plot([0,500], [0,500], 'k-')\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        #plt.xlim([-2, 18])\n",
    "        #plt.ylim([-2, 20])\n",
    "        plt.axvline(x=1.0,c='r')\n",
    "        plt.axhline(y=1.0,c='r')\n",
    "        #plt.show()\n",
    "        writer.add_figure(\"Scatter\", fig, epoch)\n",
    "        #if (epoch%5 ==0):\n",
    "        #    plt.savefig('%s/mpredVsmtrue_ep%d.png'%(plot_dir, epoch), bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        return np.mean(np.array(loss))\n",
    "        #del pred,true,loss,preda,truea,fracarr  #memtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de53d178596476ab89c8626b090f7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid_mnt/vol_home/llr/cms/cuisset/mambaforge/envs/clustering-analysis-gpu2/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nepoch):\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mepoch:\u001b[39m\u001b[39m'\u001b[39m,epoch)\n\u001b[0;32m---> 18\u001b[0m     losst\u001b[39m.\u001b[39mappend(train(epoch))\n\u001b[1;32m     19\u001b[0m     loss_epoch \u001b[39m=\u001b[39m evaluate(epoch)\n\u001b[1;32m     20\u001b[0m     lossv\u001b[39m.\u001b[39mappend(loss_epoch)\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m result \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     43\u001b[0m lossc \u001b[39m=\u001b[39m resoloss2(result, data\u001b[39m.\u001b[39mtrueBeamEnergy)\n\u001b[1;32m     44\u001b[0m loss\u001b[39m.\u001b[39mappend(lossc\u001b[39m.\u001b[39mitem()) \n",
      "File \u001b[0;32m/grid_mnt/vol_home/llr/cms/cuisset/mambaforge/envs/clustering-analysis-gpu2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m---> 10\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrn(data)\n\u001b[1;32m     11\u001b[0m     \u001b[39m#return F.softplus(logits)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/grid_mnt/vol_home/llr/cms/cuisset/mambaforge/envs/clustering-analysis-gpu2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/grid_mnt/vol_home/llr/cms/cuisset/hgcal/testbeam18/clue3d-dev/src/Plotting/ml/regression/../../ml/dynamic_reduction_network.py:119\u001b[0m, in \u001b[0;36mDynamicReductionNetwork.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    115\u001b[0m         data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputnet(data\u001b[39m.\u001b[39mx)\n\u001b[1;32m    117\u001b[0m         \u001b[39m#print(data.batch)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m#        data.x = self.batchnorm1(data.x)\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m         data\u001b[39m.\u001b[39medge_index \u001b[39m=\u001b[39m to_undirected(knn_graph(data\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk, data\u001b[39m.\u001b[39;49mbatch, loop\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, flow\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medgeconv1\u001b[39m.\u001b[39;49mflow))\n\u001b[1;32m    120\u001b[0m         data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medgeconv1(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index)\n\u001b[1;32m    122\u001b[0m         weight \u001b[39m=\u001b[39m normalized_cut_2d(data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from tqdm import notebook.tqdm as tqdm\n",
    "#import tqdm.notebook.tqdm as tqdm\n",
    "#checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_testtesttesttest_dyn2addlayer/'\n",
    "#checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_add1ip1dyn_binresoloss/'\n",
    "#\"/home/llr/cms/sghosh/GNNECAL/model_nopho_deepernet/\"\n",
    "checkpoint_dir = \"/grid_mnt/vol_home/llr/cms/cuisset/hgcal/testbeam18/clue3d-dev/src/Plotting/ml/regression/checkpoints\"\n",
    "plot_dir = \"/grid_mnt/vol_home/llr/cms/cuisset/hgcal/testbeam18/clue3d-dev/src/Plotting/ml/regression/plots\"\n",
    "nepoch=500\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_loss = 99999999\n",
    "losst = []\n",
    "\"\"\" Training loss for each epoch \"\"\"\n",
    "lossv = []\n",
    "\"\"\" Validation loss for each epoch\"\"\"\n",
    "epochs = []\n",
    "for epoch in range(nepoch):\n",
    "    print ('epoch:',epoch)\n",
    "    losst.append(train(epoch))\n",
    "    loss_epoch = evaluate(epoch)\n",
    "    lossv.append(loss_epoch)\n",
    "    epochs.append(epoch)\n",
    "    checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    checkpoint_file = 'model_epoch_%i.pth.tar' % ( epoch )\n",
    "    torch.save(checkpoint,\n",
    "                   os.path.join(checkpoint_dir,checkpoint_file ))\n",
    "    if loss_epoch < best_loss:\n",
    "        best_loss = loss_epoch\n",
    "        print('new best test loss:',best_loss)\n",
    "        torch.save(checkpoint,\n",
    "                   os.path.join(checkpoint_dir,'model_checkpoint_best.pth.tar' ))\n",
    "    \n",
    "    writer.add_scalar(\"Loss/Training\", losst[-1], epoch)\n",
    "    writer.add_scalar(\"Loss/Testing\", loss_epoch, epoch)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering-analysis-gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
